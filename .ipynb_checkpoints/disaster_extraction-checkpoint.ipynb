{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bz2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pickle\n",
    "import os\n",
    "from extraction_helpers import *\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_time_interval(df, start, end):\n",
    "    time_mask = ((df['StartDate']>=start) & (df['StartDate']<=end))\n",
    "    return df.loc[time_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_disaster(df, year_to_id_map, set_val=None):\n",
    "    IDS = sum(year_to_id_map.values(), [])\n",
    "    mask = df.index.isin(IDS)\n",
    "    df_disasters = df.loc[mask]\n",
    "    if set_val:\n",
    "        for key, value in set_val.items():\n",
    "            for key_, value_ in value.items():\n",
    "                df_disasters.at[key, key_] = value_\n",
    "    return df_disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bounding_dates(df_disaster):\n",
    "    grouped = df_disaster.groupby(df_disaster.StartDate.dt.year)\n",
    "    aggregated = grouped.agg({'StartDate':'min', 'EndDate':'max'})\n",
    "    aggregated.index.names = ['Year']\n",
    "    aggregated.columns = ['MinStartDate', 'MaxEndDate']\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_date_bounds(start, end):\n",
    "    d = datetime.timedelta(days=21)\n",
    "    lower = start - d\n",
    "    duration = end - start \n",
    "    if duration.days > 30:\n",
    "        upper = end\n",
    "    else: \n",
    "        upper = end + d\n",
    "    return lower.strftime(\"%Y-%m-%d\"), upper.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'data/emdat_processed.csv'\n",
    "parse_dates = ['StartDate', 'EndDate']\n",
    "df_emdat = pd.read_csv(data, index_col=\"Dis No\", parse_dates = parse_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "australia_heat_wave = { \n",
    "    'Group': 'Natural',\n",
    "    'Subgroup':'Meteorological', \n",
    "    'Type':'Extreme temperature ', \n",
    "    'Subtype':'Heat wave', \n",
    "    'Subsubtype': 'NaN',\n",
    "    'Name':'NaN', \n",
    "    'Country':'Australia', \n",
    "    'ISO':'AUS', \n",
    "    'Region': 'Australia and New Zealand',\n",
    "    'Continent': 'Oceania',\n",
    "    'Origin':'NaN', \n",
    "    'Magnitude':48.2, \n",
    "    'Scale':'°C', \n",
    "    'Deaths': 0,\n",
    "    'Injured':0, \n",
    "    'Affected':0,\n",
    "    'Homeless':0,\n",
    "    'TotalAffected':0, \n",
    "    'Damages':0, \n",
    "    'StartDate':'2017-01-30', \n",
    "    'EndDate': '2017-02-14',\n",
    "    'Duration':15\n",
    "}\n",
    "# Not necessary, row already added to csv file\n",
    "#row_series = pd.Series(data=australia_heat_wave, name='2017-9999-AUS')\n",
    "#df_emdat = df_emdat.append(row_series, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Group                              Natural\n",
       "Subgroup                    Meteorological\n",
       "Type                  Extreme temperature \n",
       "Subtype                          Heat wave\n",
       "Subsubtype                             NaN\n",
       "Name                                   NaN\n",
       "Country                          Australia\n",
       "ISO                                    AUS\n",
       "Region           Australia and New Zealand\n",
       "Continent                          Oceania\n",
       "Origin                                 NaN\n",
       "Magnitude                             48.2\n",
       "Scale                                   °C\n",
       "Deaths                                   0\n",
       "Injured                                  0\n",
       "Affected                                 0\n",
       "Homeless                                 0\n",
       "TotalAffected                            0\n",
       "Damages                                  0\n",
       "StartDate              2017-01-30 00:00:00\n",
       "EndDate                2017-02-14 00:00:00\n",
       "Duration                                15\n",
       "Name: 2017-9999-AUS, dtype: object"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_emdat.loc['2017-9999-AUS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORMS = {\n",
    "    '2015': ['2015-0470-MEX'],\n",
    "    '2016': ['2016-0041-FJI'],\n",
    "    '2017': ['2017-0362-USA'],\n",
    "    '2018': ['2018-0342-USA', '2018-0341-CHN', '2018-0341-PHL', '2018-0341-HKG'], # two separate storms over same time period\n",
    "    '2019': ['2019-0492-JPN'],\n",
    "    '2020': ['2020-0211-LKA', '2020-0211-BGD', '2020-0211-IND']     \n",
    "}\n",
    "\n",
    "STORMS_val = {\n",
    "    '2018-0341-CHN': {'Magnitude': 240},\n",
    "    '2018-0342-USA': {'Magnitude': 240},\n",
    "    '2020-0211-BGD': {'Magnitude': 151},\n",
    "    '2020-0211-LKA': {'Magnitude': 80}\n",
    "}\n",
    "\n",
    "HEAT_WAVES = {\n",
    "    '2015': ['2015-0189-IND'],\n",
    "    '2016': ['2016-0133-IND'],\n",
    "    '2017': ['2017-9999-AUS', '2017-0072-AUS'], # heat wave and associated fire\n",
    "    '2018': ['2018-0226-JPN', '2018-0256-PRK'],\n",
    "    '2019': ['2019-0366-BEL', '2019-0366-FRA', '2019-0366-NLD', '2019-0366-DEU'], #'2019-0366-AUT' (no value for temp), '2019-0650-GBR' (lasts too long)\n",
    "    '2020': ['2020-0441-USA'] # Forest fire\n",
    "}\n",
    "\n",
    "HEAT_WAVES_val = {\n",
    "    '2020-0441-USA': {'Magnitude': 4180},\n",
    "    '2017-0072-AUS': {'Magnitude': 550}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heat = get_df_disaster(df_emdat, HEAT_WAVES, HEAT_WAVES_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Scale</th>\n",
       "      <th>Name</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dis No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-0189-IND</th>\n",
       "      <td>48.0</td>\n",
       "      <td>°C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-05-20</td>\n",
       "      <td>2015-05-31</td>\n",
       "      <td>2248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-0133-IND</th>\n",
       "      <td>51.0</td>\n",
       "      <td>°C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-05-20</td>\n",
       "      <td>300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-9999-AUS</th>\n",
       "      <td>48.2</td>\n",
       "      <td>°C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-01-30</td>\n",
       "      <td>2017-02-14</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-0072-AUS</th>\n",
       "      <td>550.0</td>\n",
       "      <td>Km2</td>\n",
       "      <td>Sir Ivan fire</td>\n",
       "      <td>2017-02-09</td>\n",
       "      <td>2017-02-13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-0226-JPN</th>\n",
       "      <td>41.0</td>\n",
       "      <td>°C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>2018-07-15</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-0256-PRK</th>\n",
       "      <td>38.0</td>\n",
       "      <td>°C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-11</td>\n",
       "      <td>2018-08-03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-0366-BEL</th>\n",
       "      <td>41.0</td>\n",
       "      <td>°C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>2019-07-27</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-0366-FRA</th>\n",
       "      <td>44.0</td>\n",
       "      <td>°C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-21</td>\n",
       "      <td>2019-07-27</td>\n",
       "      <td>868.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-0366-NLD</th>\n",
       "      <td>40.0</td>\n",
       "      <td>°C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-22</td>\n",
       "      <td>2019-07-27</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-0366-DEU</th>\n",
       "      <td>42.0</td>\n",
       "      <td>°C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-24</td>\n",
       "      <td>2019-07-25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-0441-USA</th>\n",
       "      <td>4180.0</td>\n",
       "      <td>Km2</td>\n",
       "      <td>August Complex fire</td>\n",
       "      <td>2020-08-16</td>\n",
       "      <td>2020-10-01</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Magnitude Scale                 Name  StartDate    EndDate  \\\n",
       "Dis No                                                                      \n",
       "2015-0189-IND       48.0    °C                  NaN 2015-05-20 2015-05-31   \n",
       "2016-0133-IND       51.0    °C                  NaN 2016-04-01 2016-05-20   \n",
       "2017-9999-AUS       48.2    °C                  NaN 2017-01-30 2017-02-14   \n",
       "2017-0072-AUS      550.0   Km2        Sir Ivan fire 2017-02-09 2017-02-13   \n",
       "2018-0226-JPN       41.0    °C                  NaN 2018-07-01 2018-07-15   \n",
       "2018-0256-PRK       38.0    °C                  NaN 2018-07-11 2018-08-03   \n",
       "2019-0366-BEL       41.0    °C                  NaN 2019-07-19 2019-07-27   \n",
       "2019-0366-FRA       44.0    °C                  NaN 2019-07-21 2019-07-27   \n",
       "2019-0366-NLD       40.0    °C                  NaN 2019-07-22 2019-07-27   \n",
       "2019-0366-DEU       42.0    °C                  NaN 2019-07-24 2019-07-25   \n",
       "2020-0441-USA     4180.0   Km2  August Complex fire 2020-08-16 2020-10-01   \n",
       "\n",
       "               Deaths  \n",
       "Dis No                 \n",
       "2015-0189-IND  2248.0  \n",
       "2016-0133-IND   300.0  \n",
       "2017-9999-AUS     0.0  \n",
       "2017-0072-AUS     0.0  \n",
       "2018-0226-JPN   119.0  \n",
       "2018-0256-PRK     0.0  \n",
       "2019-0366-BEL   400.0  \n",
       "2019-0366-FRA   868.0  \n",
       "2019-0366-NLD   400.0  \n",
       "2019-0366-DEU     0.0  \n",
       "2020-0441-USA    32.0  "
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heat[['Magnitude', 'Scale', 'Name', 'StartDate', 'EndDate','Deaths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heat_bounds = retrieve_bounding_dates(df_heat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinStartDate</th>\n",
       "      <th>MaxEndDate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2015-05-20</td>\n",
       "      <td>2015-05-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>2016-05-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2017-01-30</td>\n",
       "      <td>2017-02-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>2018-08-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>2019-07-19</td>\n",
       "      <td>2019-07-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2020-08-16</td>\n",
       "      <td>2020-10-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MinStartDate MaxEndDate\n",
       "Year                        \n",
       "2015   2015-05-20 2015-05-31\n",
       "2016   2016-04-01 2016-05-20\n",
       "2017   2017-01-30 2017-02-14\n",
       "2018   2018-07-01 2018-08-03\n",
       "2019   2019-07-19 2019-07-27\n",
       "2020   2020-08-16 2020-10-01"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_heat_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storm = get_df_disaster(df_emdat, STORMS, STORMS_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storm_bounds = retrieve_bounding_dates(df_storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Magnitude</th>\n",
       "      <th>Scale</th>\n",
       "      <th>Name</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dis No</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-0470-MEX</th>\n",
       "      <td>270.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Hurricane Patricia</td>\n",
       "      <td>2015-10-22</td>\n",
       "      <td>2015-10-28</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-0041-FJI</th>\n",
       "      <td>325.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Cyclone Winston</td>\n",
       "      <td>2016-02-20</td>\n",
       "      <td>2016-02-21</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-0362-USA</th>\n",
       "      <td>215.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Hurricane Harvey</td>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-0341-CHN</th>\n",
       "      <td>240.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Typhoon Mangkut (Ompong)</td>\n",
       "      <td>2018-09-10</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-0342-USA</th>\n",
       "      <td>240.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Hurricane Florence</td>\n",
       "      <td>2018-09-12</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-0341-PHL</th>\n",
       "      <td>240.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Typhoon Mangkut (Ompong)</td>\n",
       "      <td>2018-09-16</td>\n",
       "      <td>2018-09-16</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-0341-HKG</th>\n",
       "      <td>240.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Typhoon Mangkut (Ompong)</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>2018-09-17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-0492-JPN</th>\n",
       "      <td>160.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Tropical cylone 'Hagibis'</td>\n",
       "      <td>2019-10-12</td>\n",
       "      <td>2019-10-17</td>\n",
       "      <td>99.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-0211-LKA</th>\n",
       "      <td>80.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Cyclone 'Amphan'</td>\n",
       "      <td>2020-05-17</td>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-0211-BGD</th>\n",
       "      <td>151.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Cyclone 'Amphan'</td>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-0211-IND</th>\n",
       "      <td>185.0</td>\n",
       "      <td>Kph</td>\n",
       "      <td>Cyclone 'Amphan'</td>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>2020-05-20</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Magnitude Scale                       Name  StartDate  \\\n",
       "Dis No                                                                 \n",
       "2015-0470-MEX      270.0   Kph         Hurricane Patricia 2015-10-22   \n",
       "2016-0041-FJI      325.0   Kph            Cyclone Winston 2016-02-20   \n",
       "2017-0362-USA      215.0   Kph           Hurricane Harvey 2017-08-25   \n",
       "2018-0341-CHN      240.0   Kph   Typhoon Mangkut (Ompong) 2018-09-10   \n",
       "2018-0342-USA      240.0   Kph         Hurricane Florence 2018-09-12   \n",
       "2018-0341-PHL      240.0   Kph   Typhoon Mangkut (Ompong) 2018-09-16   \n",
       "2018-0341-HKG      240.0   Kph   Typhoon Mangkut (Ompong) 2018-09-17   \n",
       "2019-0492-JPN      160.0   Kph  Tropical cylone 'Hagibis' 2019-10-12   \n",
       "2020-0211-LKA       80.0   Kph           Cyclone 'Amphan' 2020-05-17   \n",
       "2020-0211-BGD      151.0   Kph           Cyclone 'Amphan' 2020-05-20   \n",
       "2020-0211-IND      185.0   Kph           Cyclone 'Amphan' 2020-05-20   \n",
       "\n",
       "                 EndDate  Deaths  \n",
       "Dis No                            \n",
       "2015-0470-MEX 2015-10-28    14.0  \n",
       "2016-0041-FJI 2016-02-21    45.0  \n",
       "2017-0362-USA 2017-08-29    88.0  \n",
       "2018-0341-CHN 2018-09-18     0.0  \n",
       "2018-0342-USA 2018-09-18    53.0  \n",
       "2018-0341-PHL 2018-09-16    84.0  \n",
       "2018-0341-HKG 2018-09-17     0.0  \n",
       "2019-0492-JPN 2019-10-17    99.0  \n",
       "2020-0211-LKA 2020-05-20     4.0  \n",
       "2020-0211-BGD 2020-05-20    26.0  \n",
       "2020-0211-IND 2020-05-20    90.0  "
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_storm[['Magnitude', 'Scale', 'Name', 'StartDate', 'EndDate','Deaths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MinStartDate</th>\n",
       "      <th>MaxEndDate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>2015-10-22</td>\n",
       "      <td>2015-10-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>2016-02-20</td>\n",
       "      <td>2016-02-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>2017-08-25</td>\n",
       "      <td>2017-08-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>2018-09-10</td>\n",
       "      <td>2018-09-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>2019-10-12</td>\n",
       "      <td>2019-10-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>2020-05-17</td>\n",
       "      <td>2020-05-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     MinStartDate MaxEndDate\n",
       "Year                        \n",
       "2015   2015-10-22 2015-10-28\n",
       "2016   2016-02-20 2016-02-21\n",
       "2017   2017-08-25 2017-08-29\n",
       "2018   2018-09-10 2018-09-18\n",
       "2019   2019-10-12 2019-10-17\n",
       "2020   2020-05-17 2020-05-20"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_storm_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2019\n",
    "nrows = 22000000\n",
    "chunksize = 100000\n",
    "compression = 'bz2'\n",
    "#regex_pattern = r'\\b([dD]egrees)\\b|\\b(celsius)\\b|\\b(waves?)\\b|\\b(heat)\\b|\\b([tT]emperatures?)\\b|\\b([hH]ot(test)?)\\b|\\b([wW]arm)\\b|\\b([wW]eather)\\b|\\b([cC]limate)\\b|\\b([cC]limate change)\\b|\\b([gG]lobal warming)\\b|\\b([gG]reenhouse)\\b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_tags = pd.DataFrame(\n",
    "    {'tags': [\n",
    "        r'\\b([dD]egrees)\\b',\n",
    "        r'\\b(celsius)\\b',\n",
    "        r'\\b(waves?)\\b',\n",
    "        r'\\b(heat)\\b',\n",
    "        r'\\b([tT]emperatures?)\\b',\n",
    "        r'\\b([hH]ot(test)?)\\b',\n",
    "        r'\\b([wW]arm)\\b',\n",
    "        r'\\b([wW]eather)\\b']\n",
    "    }\n",
    ")  \n",
    "\n",
    "climate_tags = pd.DataFrame(\n",
    "    {'tags': [\n",
    "        r'\\b([cC]limate)\\b',\n",
    "        r'\\b([cC]limate change)\\b',\n",
    "        r'\\b([gG]lobal warming)\\b',\n",
    "        r'\\b([gG]reenhouse)\\b']\n",
    "    }\n",
    ") \n",
    "\n",
    "heat_tag_list = heat_tags.tags.values.tolist()\n",
    "climate_tag_list = climate_tags.tags.values.tolist()\n",
    "all_tags = heat_tag_list + climate_tag_list\n",
    "regex_pattern = r'|'.join(all_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 937,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nrows // chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_YEAR, end_YEAR = df_heat_bounds.loc[YEAR].MinStartDate, df_heat_bounds.loc[YEAR].MaxEndDate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_YEAR, upper_YEAR = compute_date_bounds(start_YEAR, end_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quotes = pd.read_json('data/quotes-'+str(YEAR)+'.json.bz2', lines=True, compression='bz2', chunksize=1000000, nrows=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-933-bdd708b39896>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mchunk_interval_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#with pd.read_json('data/quotes-'+str(YEAR)+'.json.bz2',lines=True,compression=compression,chunksize=chunksize) as df_reader:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/quotes-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYEAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json.bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bz2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdf_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_reader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mchunk_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlower_YEAR\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mupper_YEAR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/numerical/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/numerical/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/numerical/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[1;32m    595\u001b[0m     )\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m     json_reader = JsonReader(\n\u001b[0m\u001b[1;32m    598\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0morient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/numerical/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data_from_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/numerical/lib/python3.8/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_preprocess_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    689\u001b[0m         \"\"\"\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/numerical/lib/python3.8/bz2.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_can_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/numerical/lib/python3.8/_compression.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                     \u001b[0mrawblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrawblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chunk_interval_list = []\n",
    "with pd.read_json('data/quotes-'+str(YEAR)+'.json.bz2',lines=True,compression=compression,chunksize=chunksize) as df_reader:\n",
    "    for chunk in tqdm(df_reader, total=nrows // chunksize):\n",
    "        chunk_interval = chunk[(chunk['date'] >= lower_YEAR) & (chunk['date'] <= upper_YEAR)]\n",
    "        chunk_interval_list.append(chunk_interval[chunk_interval['quotation'].str.contains(regex_pattern)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 938,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes = pd.read_json('data/quotes-'+str(YEAR)+'.json.bz2', lines=True, compression='bz2', chunksize=chunksize, nrows=nrows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 939,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.io.json._json.JsonReader at 0x7fb40e613d00>"
      ]
     },
     "execution_count": 939,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 218/220 [24:15<00:13,  6.67s/it]\n"
     ]
    }
   ],
   "source": [
    "chunk_interval_list = []\n",
    "for chunk in tqdm(quotes, total=nrows // chunksize):\n",
    "    chunk_interval = chunk[(chunk['date'] >= lower_YEAR) & (chunk['date'] <= upper_YEAR)]\n",
    "    chunk_interval_list.append(chunk_interval[chunk_interval['quotation'].str.contains(regex_pattern)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 941,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk_interval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interval = pd.concat(chunk_interval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25880"
      ]
     },
     "execution_count": 943,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 946,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_interval.to_csv('data/heat_climate_processed.bz2',index=False, compression=compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>2019-08-14-004573</td>\n",
       "      <td>an [ ideological ] inquisition that harasses a...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2019-08-14 17:00:01</td>\n",
       "      <td>1</td>\n",
       "      <td>[[None, 0.6228], [Ken Cuccinelli, 0.3772]]</td>\n",
       "      <td>[https://instinctmagazine.com/meet-ken-cuccine...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>2019-07-18-007660</td>\n",
       "      <td>brought the Fund into the 21st Century, using ...</td>\n",
       "      <td>Christine Lagarde</td>\n",
       "      <td>[Q484605]</td>\n",
       "      <td>2019-07-18 19:19:32</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Christine Lagarde, 0.7709], [None, 0.2231], ...</td>\n",
       "      <td>[https://www.devex.com/news/who-s-ebola-declar...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2316</th>\n",
       "      <td>2019-06-28-012580</td>\n",
       "      <td>Climate change represents an existential threa...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>[Q10853588]</td>\n",
       "      <td>2019-06-28 04:10:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Kamala Harris, 0.6462], [Jay Inslee, 0.1847]...</td>\n",
       "      <td>[https://www.vox.com/policy-and-politics/2019/...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>2019-08-16-054254</td>\n",
       "      <td>My husband John's favorite four-letter word is...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2019-08-16 06:51:00</td>\n",
       "      <td>3</td>\n",
       "      <td>[[None, 0.7355], [Rachael Ray, 0.2645]]</td>\n",
       "      <td>[http://www.princegeorgecitizen.com/washington...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3854</th>\n",
       "      <td>2019-07-11-020788</td>\n",
       "      <td>Hamburger and hot dog buns at Walmart and othe...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2019-07-11 12:00:30</td>\n",
       "      <td>1</td>\n",
       "      <td>[[None, 0.5619], [Cory Booker, 0.3997], [Jeffr...</td>\n",
       "      <td>[https://thebiglead.com/2019/07/11/roundup-nin...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21760194</th>\n",
       "      <td>2019-06-28-013693</td>\n",
       "      <td>cut emissions and fight climate change from th...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2019-06-28 16:39:48</td>\n",
       "      <td>2</td>\n",
       "      <td>[[None, 0.6513], [Eric Garcetti, 0.3487]]</td>\n",
       "      <td>[http://thehill.com/policy/energy-environment/...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21760397</th>\n",
       "      <td>2019-07-18-012446</td>\n",
       "      <td>Eighty-seven degrees can make you just as sick...</td>\n",
       "      <td>Adam O'Connor</td>\n",
       "      <td>[Q4679563]</td>\n",
       "      <td>2019-07-18 05:04:16</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Adam O'Connor, 0.8147], [None, 0.1853]]</td>\n",
       "      <td>[http://journalgazette.net/news/local/20190718...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21760773</th>\n",
       "      <td>2019-07-15-018460</td>\n",
       "      <td>Given the critical juncture we are at with cli...</td>\n",
       "      <td>Alice Martin</td>\n",
       "      <td>[Q31664015, Q4725986]</td>\n",
       "      <td>2019-07-15 09:57:25</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Alice Martin, 0.9452], [None, 0.0548]]</td>\n",
       "      <td>[http://www.itpro.co.uk/automation/34014/tuc-a...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21761480</th>\n",
       "      <td>2019-07-21-009702</td>\n",
       "      <td>I cannot tell if it's the high pressure system...</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>2019-07-21 23:10:14</td>\n",
       "      <td>1</td>\n",
       "      <td>[[None, 0.7136], [Padma Lakshmi, 0.2864]]</td>\n",
       "      <td>[https://www.inquisitr.com/5542629/padma-laksh...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21762533</th>\n",
       "      <td>2019-07-25-028078</td>\n",
       "      <td>I'd imagine, if it stays this hot, we're going...</td>\n",
       "      <td>Lewis Hamilton</td>\n",
       "      <td>[Q6536656, Q9673]</td>\n",
       "      <td>2019-07-25 00:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>[[Lewis Hamilton, 0.7454], [None, 0.2529], [Va...</td>\n",
       "      <td>[https://www.formula1.com/en/latest/article.ha...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25880 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    quoteID  \\\n",
       "734       2019-08-14-004573   \n",
       "1752      2019-07-18-007660   \n",
       "2316      2019-06-28-012580   \n",
       "2393      2019-08-16-054254   \n",
       "3854      2019-07-11-020788   \n",
       "...                     ...   \n",
       "21760194  2019-06-28-013693   \n",
       "21760397  2019-07-18-012446   \n",
       "21760773  2019-07-15-018460   \n",
       "21761480  2019-07-21-009702   \n",
       "21762533  2019-07-25-028078   \n",
       "\n",
       "                                                  quotation  \\\n",
       "734       an [ ideological ] inquisition that harasses a...   \n",
       "1752      brought the Fund into the 21st Century, using ...   \n",
       "2316      Climate change represents an existential threa...   \n",
       "2393      My husband John's favorite four-letter word is...   \n",
       "3854      Hamburger and hot dog buns at Walmart and othe...   \n",
       "...                                                     ...   \n",
       "21760194  cut emissions and fight climate change from th...   \n",
       "21760397  Eighty-seven degrees can make you just as sick...   \n",
       "21760773  Given the critical juncture we are at with cli...   \n",
       "21761480  I cannot tell if it's the high pressure system...   \n",
       "21762533  I'd imagine, if it stays this hot, we're going...   \n",
       "\n",
       "                    speaker                   qids                date  \\\n",
       "734                    None                     [] 2019-08-14 17:00:01   \n",
       "1752      Christine Lagarde              [Q484605] 2019-07-18 19:19:32   \n",
       "2316          Kamala Harris            [Q10853588] 2019-06-28 04:10:00   \n",
       "2393                   None                     [] 2019-08-16 06:51:00   \n",
       "3854                   None                     [] 2019-07-11 12:00:30   \n",
       "...                     ...                    ...                 ...   \n",
       "21760194               None                     [] 2019-06-28 16:39:48   \n",
       "21760397      Adam O'Connor             [Q4679563] 2019-07-18 05:04:16   \n",
       "21760773       Alice Martin  [Q31664015, Q4725986] 2019-07-15 09:57:25   \n",
       "21761480               None                     [] 2019-07-21 23:10:14   \n",
       "21762533     Lewis Hamilton      [Q6536656, Q9673] 2019-07-25 00:00:00   \n",
       "\n",
       "          numOccurrences                                             probas  \\\n",
       "734                    1         [[None, 0.6228], [Ken Cuccinelli, 0.3772]]   \n",
       "1752                   1  [[Christine Lagarde, 0.7709], [None, 0.2231], ...   \n",
       "2316                   1  [[Kamala Harris, 0.6462], [Jay Inslee, 0.1847]...   \n",
       "2393                   3            [[None, 0.7355], [Rachael Ray, 0.2645]]   \n",
       "3854                   1  [[None, 0.5619], [Cory Booker, 0.3997], [Jeffr...   \n",
       "...                  ...                                                ...   \n",
       "21760194               2          [[None, 0.6513], [Eric Garcetti, 0.3487]]   \n",
       "21760397               1          [[Adam O'Connor, 0.8147], [None, 0.1853]]   \n",
       "21760773               1           [[Alice Martin, 0.9452], [None, 0.0548]]   \n",
       "21761480               1          [[None, 0.7136], [Padma Lakshmi, 0.2864]]   \n",
       "21762533               3  [[Lewis Hamilton, 0.7454], [None, 0.2529], [Va...   \n",
       "\n",
       "                                                       urls phase  \n",
       "734       [https://instinctmagazine.com/meet-ken-cuccine...     E  \n",
       "1752      [https://www.devex.com/news/who-s-ebola-declar...     E  \n",
       "2316      [https://www.vox.com/policy-and-politics/2019/...     E  \n",
       "2393      [http://www.princegeorgecitizen.com/washington...     E  \n",
       "3854      [https://thebiglead.com/2019/07/11/roundup-nin...     E  \n",
       "...                                                     ...   ...  \n",
       "21760194  [http://thehill.com/policy/energy-environment/...     E  \n",
       "21760397  [http://journalgazette.net/news/local/20190718...     E  \n",
       "21760773  [http://www.itpro.co.uk/automation/34014/tuc-a...     E  \n",
       "21761480  [https://www.inquisitr.com/5542629/padma-laksh...     E  \n",
       "21762533  [https://www.formula1.com/en/latest/article.ha...     E  \n",
       "\n",
       "[25880 rows x 9 columns]"
      ]
     },
     "execution_count": 944,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To remove unwanted quotes based on regex pattern:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 952,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote='it makes no sense to keep on letting millions of illegal or legal immigrants flood into the United States, and to keep the tens of millions that are already here.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote='check out the huge flood migrant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 951,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word = quote\n",
    "regex_pos = re.compile(r'flood')\n",
    "regex_neg = re.compile(r'legal|migrant')\n",
    "if regex_pos.search(word) and not regex_neg.search(word):\n",
    "    print(\"match\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2 (more efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.filter(regex='my_expression').columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bbc67402def311b96fab883d2c883a39527be9a4d3d9f29179a0d00da6a6745"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
